---
date: 2025-09-09
Meeting: true
Meeting (Group): false
tags: 
aliases:
---

### Before
- 

### In Meeting
- discussed the problem with coding (almost 0 power for MM-tggd case)

### After
- Search for reference articles for crowdsourcing
	- Results / Discussion (where you note that “approaches and even terminologies… had not been standardized” and cite Good & Su \[2\], Khare et al. \[9\])
		- More recent reviews confirm this persistent lack of standardization, e.g., Grégoire et al. (2024) systematically reviewed outbreak reporting standards and revealed significant heterogeneity in methods, timing, and metadata. These findings echo our observation that although crowdsourcing applications are diverse, consensus protocols and clear quality control frameworks remain underdeveloped.
	- *Guidelines Section (A.5.3. Screening low quality participants and validating outcomes)
		- Recent empirical work supports the effectiveness of embedding QC into task design. For example, a gamified crowdsourcing system for lung ultrasound dataset labeling showed that, when feedback and filtering mechanisms are introduced, non-expert participants can achieve performance levels close to medical experts \[JMIR 2024\]. Similarly, Sharma et al. (2024) proposed a protocol fusing crowdsourced inputs with machine learning models for diagnosing developmental delays, demonstrating how hybrid approaches can address both scalability and data reliability.
- apply the methods again on new data? How to separate?